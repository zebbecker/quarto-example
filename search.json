[
  {
    "objectID": "docs/new_region.html",
    "href": "docs/new_region.html",
    "title": "New Regions",
    "section": "",
    "text": "Most custom regions will require their EPSG codes and other settings (stored in FireConsts.py:Settings) to change for runs. This doc explains how to change those settings and create a new scheduler v3 job to run on DPS.\n\n\nThere’s nothing special about changing these environment variables. All that is needed for this to work is to put a .env file in the Settings.PREPROCESSED_DIR region folder to be picked up. A couple points will describe how this all works:\n\nAll DPS jobs are kicked off via the /maap_runtime/run_dps_cli.sh script. In that file there’s a single function and line that attempts (and gracefully exists) to bring over the .env file for a DPS run:\n# TODO: this will have to be changed to be passed dynamically once we want to use other s3 buckets\ncopy_s3_object \"s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/${regnm}/.env\" ../fireatlas/.env\nWe use the package pydantic-settings in the fireatlas code to manage our settings. If you look at the class declaration in FireConts.py you’ll see this piece of config below. This tells us a few different things:\nclass Settings(BaseSettings):\n# read in all env vars prefixed with `FEDS_` they can be in a .env file\nmodel_config = {\n    \"env_file\": \".env\",\n    \"extra\": \"ignore\",\n    \"env_prefix\": \"FEDS_\",\n}\n\nif there is a .env file available read it and use the key/values there to override any defaults listed in this class\nif there are variables in the .env file that are not declared in this class do not use them\neverything in the .env that will act as an override is prefixed with FEDS_ (so to override LOCAL_PATH in the .env you would declare FEDS_LOCAL_PATH=&lt;value&gt;)\n\n\nThat’s it. Then any imports of the fireatlas package should now have these overrides including calls to python3 FireRunDaskCoordinator.py\n\n\n\nIn the future this should definitely be a more rigid (and only additive) workflow. But the fire time will need to define this more. For now there’s only a requirement to pick a decent sounding region name. The example below will use the newly created “RussiaEast” region to explain how this is done.\n\nPick a region name (e.g. “RussiaEast”)\nCreate a folder in Settings.PREPROCESSED_DIR with a .env file. You’ll probably be doing this in a MAAP or VEDA JupyterHub and depending on the bucket parameters things outside JH might be restricted. So from within JH you can use the pre-installed aws-cli tool to do this. The command below assumes you’ve already created a local .env file you’re going to copy:\n# just showing how to list bucket key contents\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/\n                       PRE BorealManualV2/\n                       PRE BorealNA/\n                       PRE CONUS/\n\n# what's the RussiaEast .env look like?\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# cat .env \nFEDS_FTYP_OPT=\"global\"\nFEDS_CONT_OPT=\"global\"\nFEDS_EPSG_CODE=6933\n\n\n# copy the .env to a new FEDSpreprocessed folder\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 cp /projects/.env s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\nupload: ./.env to s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\n\n# check that it exists \n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/\n2024-07-12 05:55:19         66 .env\nNow that the region is defined we can immediately test it out with the GH Action “manual-nrt-v3” workflow to make sure it works before we move on to scheduling it permanently. There will be more documentation about how to run this in the future. For now the only thing to mention is that the JSON encoded param input will look something like this. And remember to NOT pass it enclosed in single quotes or double quotes but rather naked:\n{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556,46.1226744036175,168.70469654881543,77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}\n\n\n\n\n\nFind an existing v3 scheduled worklow in the .github/workflows/*.yaml such as .github/workflows/schedule-conus-nrt-v3.yaml and copy it\ncp .github/workflows/schedule-conus-nrt-v3.yaml .github/workflows/schedule-russian-east-nrt-v3.yaml\nEdit the new workflow and make sure to change some of the following sections and values. Note that leaving tst or ted as [] means it will run from current year 01/01 to now:\nname: &lt;your region name&gt; nrt v3\n...\n    - name: kick off the DPS job\n   uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n   with:\n     algo_name: eis-feds-dask-coordinator-v3\n     github_ref: 1.0.0\n     username: gcorradini\n     queue: maap-dps-eis-worker-128gb\n     maap_image_env: ubuntu\n     maap_pgt_secret: ${{ secrets.MAAP_PGT }}\n     json_params: '{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556, 46.1226744036175, 168.70469654881543, 77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}'",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/new_region.html#how-dps-jobs-pick-up-custom-settings",
    "href": "docs/new_region.html#how-dps-jobs-pick-up-custom-settings",
    "title": "New Regions",
    "section": "",
    "text": "There’s nothing special about changing these environment variables. All that is needed for this to work is to put a .env file in the Settings.PREPROCESSED_DIR region folder to be picked up. A couple points will describe how this all works:\n\nAll DPS jobs are kicked off via the /maap_runtime/run_dps_cli.sh script. In that file there’s a single function and line that attempts (and gracefully exists) to bring over the .env file for a DPS run:\n# TODO: this will have to be changed to be passed dynamically once we want to use other s3 buckets\ncopy_s3_object \"s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/${regnm}/.env\" ../fireatlas/.env\nWe use the package pydantic-settings in the fireatlas code to manage our settings. If you look at the class declaration in FireConts.py you’ll see this piece of config below. This tells us a few different things:\nclass Settings(BaseSettings):\n# read in all env vars prefixed with `FEDS_` they can be in a .env file\nmodel_config = {\n    \"env_file\": \".env\",\n    \"extra\": \"ignore\",\n    \"env_prefix\": \"FEDS_\",\n}\n\nif there is a .env file available read it and use the key/values there to override any defaults listed in this class\nif there are variables in the .env file that are not declared in this class do not use them\neverything in the .env that will act as an override is prefixed with FEDS_ (so to override LOCAL_PATH in the .env you would declare FEDS_LOCAL_PATH=&lt;value&gt;)\n\n\nThat’s it. Then any imports of the fireatlas package should now have these overrides including calls to python3 FireRunDaskCoordinator.py",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/new_region.html#defining-a-new-region",
    "href": "docs/new_region.html#defining-a-new-region",
    "title": "New Regions",
    "section": "",
    "text": "In the future this should definitely be a more rigid (and only additive) workflow. But the fire time will need to define this more. For now there’s only a requirement to pick a decent sounding region name. The example below will use the newly created “RussiaEast” region to explain how this is done.\n\nPick a region name (e.g. “RussiaEast”)\nCreate a folder in Settings.PREPROCESSED_DIR with a .env file. You’ll probably be doing this in a MAAP or VEDA JupyterHub and depending on the bucket parameters things outside JH might be restricted. So from within JH you can use the pre-installed aws-cli tool to do this. The command below assumes you’ve already created a local .env file you’re going to copy:\n# just showing how to list bucket key contents\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/\n                       PRE BorealManualV2/\n                       PRE BorealNA/\n                       PRE CONUS/\n\n# what's the RussiaEast .env look like?\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# cat .env \nFEDS_FTYP_OPT=\"global\"\nFEDS_CONT_OPT=\"global\"\nFEDS_EPSG_CODE=6933\n\n\n# copy the .env to a new FEDSpreprocessed folder\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 cp /projects/.env s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\nupload: ./.env to s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\n\n# check that it exists \n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/\n2024-07-12 05:55:19         66 .env\nNow that the region is defined we can immediately test it out with the GH Action “manual-nrt-v3” workflow to make sure it works before we move on to scheduling it permanently. There will be more documentation about how to run this in the future. For now the only thing to mention is that the JSON encoded param input will look something like this. And remember to NOT pass it enclosed in single quotes or double quotes but rather naked:\n{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556,46.1226744036175,168.70469654881543,77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/new_region.html#defining-a-new-region-schedule-run",
    "href": "docs/new_region.html#defining-a-new-region-schedule-run",
    "title": "New Regions",
    "section": "",
    "text": "Find an existing v3 scheduled worklow in the .github/workflows/*.yaml such as .github/workflows/schedule-conus-nrt-v3.yaml and copy it\ncp .github/workflows/schedule-conus-nrt-v3.yaml .github/workflows/schedule-russian-east-nrt-v3.yaml\nEdit the new workflow and make sure to change some of the following sections and values. Note that leaving tst or ted as [] means it will run from current year 01/01 to now:\nname: &lt;your region name&gt; nrt v3\n...\n    - name: kick off the DPS job\n   uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n   with:\n     algo_name: eis-feds-dask-coordinator-v3\n     github_ref: 1.0.0\n     username: gcorradini\n     queue: maap-dps-eis-worker-128gb\n     maap_image_env: ubuntu\n     maap_pgt_secret: ${{ secrets.MAAP_PGT }}\n     json_params: '{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556, 46.1226744036175, 168.70469654881543, 77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}'",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/publications.html",
    "href": "docs/publications.html",
    "title": "Recent Publications",
    "section": "",
    "text": "TODO: link to various FEDS papers.",
    "crumbs": [
      "Data",
      "Recent Publications"
    ]
  },
  {
    "objectID": "docs/fireatlas_core.html",
    "href": "docs/fireatlas_core.html",
    "title": "FEDS Algorithm",
    "section": "",
    "text": "TODO: explanation of core functions from fireatlas package (FireForward, etc,).",
    "crumbs": [
      "Technical Documentation",
      "FEDS Algorithm"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html",
    "href": "docs/mapping-fires.html",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#run-this-notebook",
    "href": "docs/mapping-fires.html#run-this-notebook",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "",
    "text": "You can launch this notbook using mybinder, by clicking the button below.",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#approach",
    "href": "docs/mapping-fires.html#approach",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Approach",
    "text": "Approach\n\nUse OWSLib to determine what data is available and inspect the metadata\nUse OWSLib to filter and read the data\nUse geopandas and folium to analyze and plot the data\n\nNote that the default examples environment is missing one requirement: oswlib. We can pip install that before we move on.\n\n%pip install OWSLib==0.28.1 --quiet\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport datetime as dt\n\nimport geopandas as gpd\nfrom owslib.ogcapi.features import Features",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#about-the-data",
    "href": "docs/mapping-fires.html#about-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "About the Data",
    "text": "About the Data\nThe fire data shown is generated by the FEDs algorithm. The FEDs algorithm tracks fire movement and severity by ingesting observations from the VIIRS thermal sensors on the Suomi NPP and NOAA-20 satellites. This algorithm uses raw VIIRS observations to generate a polygon of the fire, locations of the active fire line, and estimates of fire mean Fire Radiative Power (FRP). The VIIRS sensors overpass at ~1:30 AM and PM local time, and provide estimates of fire evolution ~ every 12 hours. The data produced by this algorithm describe where fires are in space and how fires evolve through time. This CONUS-wide implementation of the FEDs algorithm is based on Chen et al 2020’s algorithm for California.\nThe data produced by this algorithm is considered experimental.",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "href": "docs/mapping-fires.html#look-at-the-data-that-is-availible-through-the-ogc-api",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Look at the data that is availible through the OGC API",
    "text": "Look at the data that is availible through the OGC API\nThe datasets that are distributed throught the OGC API are organized into collections. We can display the collections with the command:\n\nOGC_URL = \"https://firenrt.delta-backend.com\"\n\nw = Features(url=OGC_URL)\nw.feature_collections()\n\n['public.eis_fire_lf_perimeter_archive',\n 'public.eis_fire_lf_newfirepix_archive',\n 'public.eis_fire_lf_fireline_archive',\n 'public.eis_fire_lf_fireline_nrt',\n 'public.eis_fire_snapshot_fireline_nrt',\n 'public.eis_fire_snapshot_newfirepix_nrt',\n 'public.eis_fire_lf_newfirepix_nrt',\n 'public.eis_fire_perimeter',\n 'public.eis_fire_lf_perimeter_nrt',\n 'public.eis_fire_snapshot_perimeter_nrt',\n 'public.st_squaregrid',\n 'public.st_hexagongrid',\n 'public.st_subdivide']\n\n\nWe will focus on the public.eis_fire_snapshot_fireline_nrt collection, the public.eis_fire_snapshot_perimeter_nrt collection, and the public.eis_fire_lf_perimeter_archive collection here.\n\nInspect the metatdata for public.eis_fire_snapshot_perimeter_nrt collection\nWe can access information that describes the public.eis_fire_snapshot_perimeter_nrt table.\n\nperm = w.collection(\"public.eis_fire_snapshot_perimeter_nrt\")\n\nWe are particularly interested in the spatial and temporal extents of the data.\n\nperm[\"extent\"]\n\n{'spatial': {'bbox': [[-164.04434204101562,\n    24.15606689453125,\n    163.10562133789062,\n    70.45816802978516]],\n  'crs': 'http://www.opengis.net/def/crs/OGC/1.3/CRS84'},\n 'temporal': {'interval': [['2024-07-17T12:00:00+00:00',\n    '2024-08-11T00:00:00+00:00']],\n  'trs': 'http://www.opengis.net/def/uom/ISO-8601/0/Gregorian'}}\n\n\nIn addition to getting metadata about the data we can access the queryable fields. Each of these fields will represent a column in our dataframe.\n\nperm_q = w.collection_queryables(\"public.eis_fire_snapshot_perimeter_nrt\")\nperm_q[\"properties\"]\n\n{'geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'duration': {'name': 'duration', 'type': 'number'},\n 'farea': {'name': 'farea', 'type': 'number'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'flinelen': {'name': 'flinelen', 'type': 'number'},\n 'fperim': {'name': 'fperim', 'type': 'number'},\n 'geom_counts': {'name': 'geom_counts', 'type': 'string'},\n 'isactive': {'name': 'isactive', 'type': 'number'},\n 'low_confidence_grouping': {'name': 'low_confidence_grouping',\n  'type': 'number'},\n 'meanfrp': {'name': 'meanfrp', 'type': 'number'},\n 'n_newpixels': {'name': 'n_newpixels', 'type': 'number'},\n 'n_pixels': {'name': 'n_pixels', 'type': 'number'},\n 'pixden': {'name': 'pixden', 'type': 'number'},\n 'primarykey': {'name': 'primarykey', 'type': 'string'},\n 'region': {'name': 'region', 'type': 'string'},\n 't': {'name': 't', 'type': 'string'}}",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#filter-the-data",
    "href": "docs/mapping-fires.html#filter-the-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Filter the data",
    "text": "Filter the data\nIt is always a good idea to do any data filtering as early as possible. In this example we know that we want the data for particular spatial and temporal extents. We can apply those and other filters using the OWSLib package.\nIn the below example we are:\n\nchoosing the public.eis_fire_snapshot_perimeter_nrt collection\nsubsetting it by space using the bbox parameter\nsubsetting it by time using the datetime parameter\nfiltering for fires over 5km^2 and over 2 days long using the filter parameter. The filter parameter lets us filter by the columns in ‘public.eis_fire_snapshot_perimeter_nrt’ using SQL-style queries.\n\nNOTE: The limit parameter desginates the maximum number of objects the query will return. The default limit is 10, so if we want to all of the fire perimeters within certain conditions, we need to make sure that the limit is large.\n\n## Get the most recent fire perimeters, and 7 days before most recent fire perimeter\nmost_recent_time = max(*perm[\"extent\"][\"temporal\"][\"interval\"])\nnow = dt.datetime.strptime(most_recent_time, \"%Y-%m-%dT%H:%M:%S+00:00\")\nlast_week = now - dt.timedelta(weeks=1)\nlast_week = dt.datetime.strftime(last_week, \"%Y-%m-%dT%H:%M:%S+00:00\")\nprint(\"Most Recent Time =\", most_recent_time)\nprint(\"Last week =\", last_week)\n\nMost Recent Time = 2024-08-11T00:00:00+00:00\nLast week = 2024-08-04T00:00:00+00:00\n\n\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",  # name of the dataset we want\n    bbox=[\"-106.8\", \"24.5\", \"-72.9\", \"37.3\"],  # coodrinates of bounding box,\n    datetime=[last_week + \"/\" + most_recent_time],  # date range\n    limit=1000,  # max number of items returned\n    filter=\"farea&gt;5 AND duration&gt;2\",  # additional filters based on queryable fields\n)\n\nThe result is a dictionary containing all of the data and some summary fields. We can look at the keys to see what all is in there.\n\nperm_results.keys()\n\ndict_keys(['type', 'id', 'title', 'description', 'numberMatched', 'numberReturned', 'links', 'features'])\n\n\nFor instance you can check the total number of matched items and make sure that it is equal to the number of returned items. This is how you know that the limit you defined above is high enough.\n\nperm_results[\"numberMatched\"] == perm_results[\"numberReturned\"]\n\nTrue\n\n\nYou can also access the data directly in the browser or in an HTTP GET call using the constructed link.\n\nperm_results[\"links\"][1][\"href\"]\n\n'https://firenrt.delta-backend.com/collections/public.eis_fire_snapshot_perimeter_nrt/items?bbox=-106.8%2C24.5%2C-72.9%2C37.3&datetime=2024-08-04T00%3A00%3A00%2B00%3A00%2F2024-08-11T00%3A00%3A00%2B00%3A00&limit=1000&filter=farea%3E5+AND+duration%3E2'",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "href": "docs/mapping-fires.html#visualize-most-recent-fire-perimeters-with-firelines",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize Most Recent Fire Perimeters with Firelines",
    "text": "Visualize Most Recent Fire Perimeters with Firelines\nIf we wanted to combine collections to make more informative analyses, we can use some of the same principles.\nFirst we’ll get the queryable fields, and the extents:\n\nfline_q = w.collection_queryables(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_collection = w.collection(\"public.eis_fire_snapshot_fireline_nrt\")\nfline_q[\"properties\"]\n\n{'geometry': {'$ref': 'https://geojson.org/schema/Geometry.json'},\n 'fireid': {'name': 'fireid', 'type': 'number'},\n 'mergeid': {'name': 'mergeid', 'type': 'number'},\n 'primarykey': {'name': 'primarykey', 'type': 'string'},\n 'region': {'name': 'region', 'type': 'string'},\n 't': {'name': 't', 'type': 'string'}}\n\n\n\nRead\nThen we’ll use those fields to get most recent fire perimeters and fire lines.\n\nperm_results = w.collection_items(\n    \"public.eis_fire_snapshot_perimeter_nrt\",\n    datetime=most_recent_time,\n    limit=1000,\n)\nperimeters = gpd.GeoDataFrame.from_features(perm_results[\"features\"])\n\n## Get the most recent fire lines\nperimeter_ids = perimeters.fireid.unique()\nperimeter_ids = \",\".join(map(str, perimeter_ids))\n\nfline_results = w.collection_items(\n    \"public.eis_fire_snapshot_fireline_nrt\",\n    limit=1000,\n    filter=\"fireid IN (\"\n    + perimeter_ids\n    + \")\",  # only the fires from the fire perimeter query above\n)\nfline = gpd.GeoDataFrame.from_features(fline_results[\"features\"])\n\n\n\nVisualize\n\nperimeters = perimeters.set_crs(\"epsg:4326\")\nfline = fline.set_crs(\"epsg:4326\")\n\nm = perimeters.explore(zoom_start=5, location=(41, -122))\nm = fline.explore(m=m, color=\"orange\")\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#visualize-the-growth-of-the-camp-fire",
    "href": "docs/mapping-fires.html#visualize-the-growth-of-the-camp-fire",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Visualize the Growth of the Camp Fire",
    "text": "Visualize the Growth of the Camp Fire\nWe may be interested in understanding how a fire evolved through time. To do this, we can work with the “Large fire” or “lf” perimeter collections. The public.eis_fire_lf_perimeter_nrt collection has the full spread history of fires from this year. public.eis_fire_lf_perimeter_archive has the full spread history of fires from 2018-2021 that were in the Western United States. The Camp Fire was in 2018, so we will work with the public.eis_fire_lf_perimeter_archive collection.\nWe can start by querying with information specific to the Camp Fire, like it’s genreal region (Northern California), and when it was active (November 2018). With that information, we can get the fireID associated with the Camp Fire.\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",\n    bbox=[\"-124.52\", \"39.2\", \"-120\", \"42\"],  # North California bounding box,\n    datetime=[\"2018-11-01T00:00:00+00:00/2018-11-30T12:00:00+00:00\"],\n    limit=3000,\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by=\"t\", ascending=False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\nprint(perimeters.fireid.unique())\nm = perimeters.explore(\n    style_kwds={\"fillOpacity\": 0}, zoom_start=9, location=(39.7, -121.4)\n)\nm\n\n['F17028' 'F18493']\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nBased on the map, we know that the fireID for the Camp Fire is “F17028”. We can use that to directly query for that particular fire.\n\nperimeters_archive_results = w.collection_items(\n    \"public.eis_fire_lf_perimeter_archive\",\n    filter=\"fireid = 'F17028'\",\n    datetime=[\"2018-01-01T00:00:00+00:00/2018-12-31T12:00:00+00:00\"],\n    limit=3000,\n)\n\nperimeters_archive_results\n\nperimeters = gpd.GeoDataFrame.from_features(perimeters_archive_results[\"features\"])\nperimeters = perimeters.sort_values(by=\"t\", ascending=False)\nperimeters = perimeters.set_crs(\"epsg:4326\")\n\nm = perimeters.explore(\n    style_kwds={\"fillOpacity\": 0}, zoom_start=12, location=(39.7, -121.4)\n)\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#download-data",
    "href": "docs/mapping-fires.html#download-data",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Download Data",
    "text": "Download Data\nDownloading pre-filtered data may be useful for working locally, or for working with the data in GIS software.\nWe can download the dataframe we made by writing it out into a shapefile or into a GeoJSON file.\nperimeters.to_file('perimeters.shp') \nperimeters.to_file('perimeters.geojson', driver='GeoJSON')",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/mapping-fires.html#collection-information",
    "href": "docs/mapping-fires.html#collection-information",
    "title": "Get Fire Perimeters from an OGC API",
    "section": "Collection Information",
    "text": "Collection Information\nThe API hosts 9 different collections. There are four different types of data, and three different time-scales availible for querying through the API. “*snapshot*” collections are useful for visualizing the most recent data. It contains the most recent fires perimeters, active firelines, or VIIRS observations within the last 20 days. “*lf*” collections (short for Large Fire), show every fire perimeter, active fire line, or VIIRS observations for fires over 5 km^2. Collections that end in *archive are for year 2018 - 2021 across the Western United States. Collections with the *nrt ending are for CONUS from this most recent year. FireIDs are consistent only between layers with the same timescale (snapshot, lf_*nrt, and lf_archive*).\npublic.eis_fire_snapshot_perimeter_nrt\nPerimeter of cumulative fire-area. Most recent perimeter from the last 20 days.\npublic.eis_fire_lf_perimeter_nrt\nPerimeter of cumulative fire-area, from fires over 5 km^2. Every fire perimeter from current year to date.\npublic.eis_fire_lf_perimeter_archive\nPerimeter of cumulative fire-area, from fires over 5 km^2 in the Western United States. Every fire perimeter from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nmeanfrp\nMean fire radiative power. The weighted sum of the fire radiative power detected at each new pixel, divided by the number of pixels. If no new pixels are detected, meanfrp is set to zero.\nMW/(pixel area)\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nfireid\nFire ID. Unique for each fire. Matches fireid.\nNumeric ID\n\n\npixden\nNumber of pixels divided by area of perimeter.\npixels/Km^2\n\n\nduration\nNumber of days since first observation of fire. Fires with a single observation have a duration of zero.\nDays\n\n\nflinelen\nLength of active fire line, based on new pixels. If no new pixels are detected, flinelen is set to zero.\nKm\n\n\nfperim\nLength of fire perimeter.\nKm\n\n\nfarea\nArea within fire perimeter.\nKm^2\n\n\nn_newpixels\nNumber of pixels newly detected since last overpass.\npixels\n\n\nn_pixels\nNumber of pixel-detections in history of fire.\npixels\n\n\nisactive\nHave new fire pixels been detected in the last 5 days?\nBoolean\n\n\nogc_fid\nThe ID used by the OGC API to sort perimeters.\nNumeric ID\n\n\ngeometry\nThe shape of the perimeter.\nGeometry\n\n\n\npublic.eis_fire_snapshot_fireline_nrt\nActive fire line as estimated by new VIIRS detections. Most fire line from the last 20 days.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2. Every fire line from current year to date.\npublic.eis_fire_lf_fireline_nrt\nActive fire line as estimated by new VIIRS detections, from fires over 5 km^2 in the Western United States. Every fire line from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID\n\n\n\npublic.eis_fire_snapshot_newfirepix_nrt\nNew pixel detections that inform the most recent time-step’s perimeter and fireline calculation from the last 20 days.\npublic.eis_fire_lf_newfirepix_nrt\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible from start of current year to date.\npublic.eis_fire_lf_newfirepix_archive\nNew pixel detections that inform a given time-step’s perimeter and fireline calculation. Availible for Western United States from 2018-2021.\n\n\n\n\n\n\n\n\nColumn\nDescription\nUnit\n\n\n\n\nfireid\nID of fire pixel associated with.\nNumeric ID\n\n\nt\nTime of VIIRS detection, corrected to noon and midnight.\nDatetime. yyyy-mm-ddThh:mm:ss. Local time.\n\n\nmergeid\nID used to connect pixels to perimeters. Matches fireid\nNumeric ID\n\n\nogc_fid\nThe ID used by the OGC API to sort pixels.\nNumeric ID",
    "crumbs": [
      "Data",
      "Get Fire Perimeters from an OGC API"
    ]
  },
  {
    "objectID": "docs/playbook_dps_debug.html",
    "href": "docs/playbook_dps_debug.html",
    "title": "DPS Debugging Playbook",
    "section": "",
    "text": "DPS Debugging Playbook\nHere are a series of steps that can help find out where something is failing regardless of whether the job is \"completed\" or \"failed\" state from the DPS perspective.\n\nDPS Job UI/UX:\nAll resolution routes will eventually lead us to use the DPS Job UI/UX so let’s make sure we have this workspace set up in the ADE:\napiVersion: 1.0.0\nmetadata:\n  name: dps-job-management\nattributes:\n  editorFree: 'true'\ncomponents:\n  - endpoints:\n      - attributes:\n          type: ide\n          discoverable: 'false'\n          path: /\n          protocol: http\n          public: 'true'\n        name: jupyter\n        port: 3100\n    referenceContent: |\n      kind: List\n      items:\n        - apiVersion: v1\n          kind: Pod\n          metadata:\n            name: ws\n            labels:\n              ssh: enabled\n          spec:\n            containers:\n              - name: jupyter\n                image: 'mas.dit.maap-project.org/root/maap-workspaces/hysds-proxy:ops'\n                imagePullPolicy: Always\n                resources:\n                  limits:\n                    memory: 8096Mi\n    type: kubernetes\n\n\nI Think a Job is Failing:\nStep one, we need metadata about a job to find it in the DPS Job UI/UX above. Since GH Actions are the interface to kicking off all manual jobs or scheduled jobs then that’s where we’ll go to figure everything out.\nThere are two paths to resolution here. Both are expanded on in the sections below:\n\nfind the GH Action that kicked off the job\nfind any failing jobs that ended within the last hour using the GH Action \"schedule-alert-failed-dps-jobs\"\n\n\n\nGH Action That Kicked Off A Job\nIf used the manual GH Action to kick off a job or if the job was scheduled we can go find it in the GH Actions UI. Note that you can filter for all job runs of the job you want to find by clicking the left-handed nav highlighted in red below: \nFind the most recent scheduled job or the manual job you kicked off and then click through to see the job steps. The step that dumps out job metadata is called \"kick off DPS job\" as highlighted in red below. Also highlighted in red are the two most important pieces of metadata information to retrieve about the job: the algorithm name, algorithm version and the job id: \n\n\nGH Action Alert Failed DPS Jobs\nClick on the \"alert-failed-dps-jobs\" GH Action in the left-handed nav of actions to see most recent runs. This action runs every hour and sniffs for jobs that failed with an end time within the past hour. Actions that detect faild jobs should be red as opposed to green: \nClick on a red action and drill down to the step that dumps out job metadata is called \"filter DPS failed jobs and alert\" as highlighted in red below. Also highlighted in red are the two most important pieces of metadata information to retrieve about the job: the algorithm name, algorithm version and the job id: \n\n\nGo to DPS UI/UX And Filter For Job\nThe easiest way to find a job is using the algorithm name and version and then using the job id to verify its correct. Go to the DPS Job UI/UX and \nIf a DPS job is in the \"failed\" state it should have a clear traceback listed in the job card. If it is in the \"completed\" state then you should be able to click through in the job card under \"View Job\" and look at the _stderr.txt and running.log in outputs for more information",
    "crumbs": [
      "Technical Documentation",
      "DPS Debugging Playbook"
    ]
  },
  {
    "objectID": "docs/releasing.html",
    "href": "docs/releasing.html",
    "title": "Releasing",
    "section": "",
    "text": "In this context “releasing” means the folloing things:\n\ntagging the algorithm with a certain semantic version (semver for short)\nbuild and publishing fireatlas on PyPI\nbuilding an image off that tag that will be used in some async task runner (currently only DPS) to run the regional algorithm jobs asynchronously.\n\nMost of this can be automated but since semver is often about considering if the newest set of changes we are packaging up under a version is backward compatible it does require a human to choose the version.\nThat said, the fireatlas code isn’t a library that others will be using in their code and so that also relieves us of considering backward incompatible changes. Therefore, we will probably never increment the major release number and only minor or patch in &lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;. Another way of saying that is we fireatlas will always be releasing within the &gt;=1.0.0,&lt;2.0.0 range\n\n\n\nThe releaser should look at the current release tags and versions and decide if the minor or patch version should be incremented:\n\nare all the merged changes in this release just bug fixes? then bump the patch (1.&lt;minor&gt;.&lt;patch&gt;) version by one\ndid any of the merged changes going out include new features? then bump the minor (1.&lt;minor&gt;.&lt;patch&gt;) version by one\n\n\n\n\nOnce the releaser has a version number, then will need to create a PR that modifies version in a couple places:\n\nthe algorithm config algorithm_version in ./maap_runtime/coordinator/algorithm_config.yaml:\n\nalgorithm_description: \"coordinator for all regional jobs, preprocess and FireForward steps\"\nalgorithm_version: &lt;NEW VERSION NUMBER HERE&gt;\nenvironment: ubuntu\n\nunfortunately all the scheduled jobs also pass this version to kick off jobs and therefore also need to be updated in ./.github/workflows/schedule-*.yaml:\n\n- name: kick off the DPS job\n  uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n  with:\n    algo_name: eis-feds-dask-coordinator-v3\n    github_ref: &lt;NEW VERSION NUMBER HERE&gt;\n    username: gcorradini\n\n\n\nThe releaser can merge the above PR and then kick off a new release by doing the following:\n\nGo to https://github.com/Earth-Information-System/fireatlas/releases\nclick “Draft New Release”\ncreate a new tag for this release that matches the version chosen above\nclick the “Generate release notes”\nreview the release notes and clean up\nclick the “Publish release”\n\n\n\n\nThe manual step in the last section will kick off an async GH actions workflow that does the following\n\nuses our version information and builds a python package using twine\npublishes the package to PyPI with that version number\nkicks off a DPS job that builds a new image\n\n\n\n\nThe biggest thing that can wrong with this workflow is that the DPS image builder fails to build our image. Then the algorithm will not be running the newest code. In the GH release action job you should be able see in the logs where the DPS image job is building and check the status:\n# EXAMPLE LOG\n{\"code\": 200, \"message\": {\"id\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"short_id\": \"ec3202d4\", \"created_at\": \"2024-07-30T20:34:28.000+00:00\", \"parent_ids\": [\"91dfb3a4edff20c7049825101f015b67c8a05d3a\"], \"title\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"message\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"author_name\": \"root\", \"author_email\": \"root@845666954fdb\", \"authored_date\": \"2024-07-30T20:34:28.000+00:00\", \"committer_name\": \"root\", \"committer_email\": \"root@845666954fdb\", \"committed_date\": \"2024-07-30T20:34:28.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/commit/ec3202d4adeb02f7d887d88d2af9784184e60344\", \"stats\": {\"additions\": 7, \"deletions\": 7, \"total\": 14}, \"status\": \"created\", \"project_id\": 3, \"last_pipeline\": {\"id\": 14293, \"iid\": 1332, \"project_id\": 3, \"sha\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"ref\": \"main\", \"status\": \"created\", \"source\": \"push\", \"created_at\": \"2024-07-30T20:34:29.737Z\", \"updated_at\": \"2024-07-30T20:34:29.737Z\", \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/pipelines/14293\"}, \"job_web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578\", \"job_log_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578/raw\"}}\nPlease verify that the job succeeds or you’ll have to manually trigger an image build again via ./maap_runtime/register-all.ipynb. To verify that the job succeeded, you can view the DPS job’s progress in GitLab at the job_web_url provided in the response from DPS as shown above.",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#choose-a-version-number",
    "href": "docs/releasing.html#choose-a-version-number",
    "title": "Releasing",
    "section": "",
    "text": "The releaser should look at the current release tags and versions and decide if the minor or patch version should be incremented:\n\nare all the merged changes in this release just bug fixes? then bump the patch (1.&lt;minor&gt;.&lt;patch&gt;) version by one\ndid any of the merged changes going out include new features? then bump the minor (1.&lt;minor&gt;.&lt;patch&gt;) version by one",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#create-a-new-pr-for-dps-jobs",
    "href": "docs/releasing.html#create-a-new-pr-for-dps-jobs",
    "title": "Releasing",
    "section": "",
    "text": "Once the releaser has a version number, then will need to create a PR that modifies version in a couple places:\n\nthe algorithm config algorithm_version in ./maap_runtime/coordinator/algorithm_config.yaml:\n\nalgorithm_description: \"coordinator for all regional jobs, preprocess and FireForward steps\"\nalgorithm_version: &lt;NEW VERSION NUMBER HERE&gt;\nenvironment: ubuntu\n\nunfortunately all the scheduled jobs also pass this version to kick off jobs and therefore also need to be updated in ./.github/workflows/schedule-*.yaml:\n\n- name: kick off the DPS job\n  uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n  with:\n    algo_name: eis-feds-dask-coordinator-v3\n    github_ref: &lt;NEW VERSION NUMBER HERE&gt;\n    username: gcorradini",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#merge-pr-and-manually-release",
    "href": "docs/releasing.html#merge-pr-and-manually-release",
    "title": "Releasing",
    "section": "",
    "text": "The releaser can merge the above PR and then kick off a new release by doing the following:\n\nGo to https://github.com/Earth-Information-System/fireatlas/releases\nclick “Draft New Release”\ncreate a new tag for this release that matches the version chosen above\nclick the “Generate release notes”\nreview the release notes and clean up\nclick the “Publish release”",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#release-publish-workflow",
    "href": "docs/releasing.html#release-publish-workflow",
    "title": "Releasing",
    "section": "",
    "text": "The manual step in the last section will kick off an async GH actions workflow that does the following\n\nuses our version information and builds a python package using twine\npublishes the package to PyPI with that version number\nkicks off a DPS job that builds a new image",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#verify-dps-image-build",
    "href": "docs/releasing.html#verify-dps-image-build",
    "title": "Releasing",
    "section": "",
    "text": "The biggest thing that can wrong with this workflow is that the DPS image builder fails to build our image. Then the algorithm will not be running the newest code. In the GH release action job you should be able see in the logs where the DPS image job is building and check the status:\n# EXAMPLE LOG\n{\"code\": 200, \"message\": {\"id\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"short_id\": \"ec3202d4\", \"created_at\": \"2024-07-30T20:34:28.000+00:00\", \"parent_ids\": [\"91dfb3a4edff20c7049825101f015b67c8a05d3a\"], \"title\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"message\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"author_name\": \"root\", \"author_email\": \"root@845666954fdb\", \"authored_date\": \"2024-07-30T20:34:28.000+00:00\", \"committer_name\": \"root\", \"committer_email\": \"root@845666954fdb\", \"committed_date\": \"2024-07-30T20:34:28.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/commit/ec3202d4adeb02f7d887d88d2af9784184e60344\", \"stats\": {\"additions\": 7, \"deletions\": 7, \"total\": 14}, \"status\": \"created\", \"project_id\": 3, \"last_pipeline\": {\"id\": 14293, \"iid\": 1332, \"project_id\": 3, \"sha\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"ref\": \"main\", \"status\": \"created\", \"source\": \"push\", \"created_at\": \"2024-07-30T20:34:29.737Z\", \"updated_at\": \"2024-07-30T20:34:29.737Z\", \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/pipelines/14293\"}, \"job_web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578\", \"job_log_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578/raw\"}}\nPlease verify that the job succeeds or you’ll have to manually trigger an image build again via ./maap_runtime/register-all.ipynb. To verify that the job succeeded, you can view the DPS job’s progress in GitLab at the job_web_url provided in the response from DPS as shown above.",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/api.html",
    "href": "docs/api.html",
    "title": "API",
    "section": "",
    "text": "Hello world!"
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "NASA Wildfire Tracking Lab",
    "section": "",
    "text": "Spread of the Dixie fire between July 14 and October 22, 2021, with the fire line for each 12-hour step in time shown in a different color.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/index.html#our-work",
    "href": "docs/index.html#our-work",
    "title": "NASA Wildfire Tracking Lab",
    "section": "Our Work",
    "text": "Our Work\nShort, 1 paragraph summary of team’s work, vision.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/index.html#resources",
    "href": "docs/index.html#resources",
    "title": "NASA Wildfire Tracking Lab",
    "section": "Resources",
    "text": "Resources\nOur team and collaborators develop and maintain the Fire Event Data Suite, or FEDS. The FEDS code is open source and available on GitHub.\nWith FEDS, we continously generate subdaily estimated fire perimeters using data from the VIIRS sensors on Suomi-NPP and NOAA20 satellites. We currently make data available for several regions of interest, including the continental United States, on our public API.\nResearchers interested in exploring our data can read more about our data products and recent publications, and are welcome to contact us for further discussion.\nDevelopers should consult our technical documentation and engage with our community on GitHub.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/index.html#team",
    "href": "docs/index.html#team",
    "title": "NASA Wildfire Tracking Lab",
    "section": "Team",
    "text": "Team\nTeam and affiliations.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "docs/data.html",
    "href": "docs/data.html",
    "title": "Data Products",
    "section": "",
    "text": "Disclaimer: The VIIRS Modeled Fire Perimeters product is intended to provide situational awareness for ongoing fire events in the US and Canada, not a precise estimate of the fire perimeter for emergency response. This research product estimates individual fire event perimeters and properties every 12 hours; the spatial and temporal resolution of each modeled fire perimeter reflects the characteristics and availability of VIIRS 375 m active fire detections from FIRMS.",
    "crumbs": [
      "Data",
      "Data Products"
    ]
  },
  {
    "objectID": "docs/data.html#viirs-modeled-fire-perimeters",
    "href": "docs/data.html#viirs-modeled-fire-perimeters",
    "title": "Data Products",
    "section": "VIIRS Modeled Fire Perimeters",
    "text": "VIIRS Modeled Fire Perimeters\nThe VIIRS Modeled Fire Perimeters product uses the Fire Event Data Suite (FEDS) algorithm (Chen et al., 2022), an alpha shape approach to estimate the perimeter and properties of ongoing fire events every 12 hours, based on Visible Infrared Imaging Radiometer Suite (VIIRS) 375 m active fire detections (hereafter “modeled fire perimeters”).\nThe FEDS algorithm clusters active fire detections into individual events, estimates the fire perimeter and fire characteristics, and tracks fire growth for each 12-hour time step. The vector data product includes multiple attributes for each modeled perimeter, including the active portion of the fire perimeter and metrics of fire behavior for each 12-hour growth increment. The FEDS algorithm inherits the spatial resolution and geolocation accuracy of the VIIRS 375 m active fire product from the Suomi-NPP and NOAA-20 satellites (see Disclaimer, above) and models the fire perimeter every 12 hours to generate a vector output. The VIIRS Modeled Fire Perimeter data are produced within approximately four hours of each VIIRS overpass for the Continental United States (CONUS) and Canada.\nFor all fire events, regardless of fire size or duration, the FEDS algorithm provides a “snapshot” of the modeled fire perimeter every 12 hours. For fires that ultimately grow larger than 5 km2, these modeled perimeters and associated attributes are stored in sequence to track the growth and behavior of each large fire event.\nThe source code used to generate the VIIRS Modeled Fire Perimeter data using the FEDS algorithm is available on GitHub, and an example notebook that demonstrates how to subset, analyze, and visualize the data outside of FIRMS is available on NASA’s VEDA platform. Finally, the VIIRS Modeled Fire Perimeter data based on the FEDS algorithm are also available via OGC API.\nThe VIIRS Modeled fire Perimeter data are provided by the NASA Earth Information System (EIS) Project. The FEDS algorithm was developed by Yang Chen and colleagues at the University of California-Irvine, NASA’s Goddard Space Flight Center (GSFC), Cardiff University (UK), and Universidad del Rosario (Colombia), as described in a scientific paper published in 2022 (Chen et al., 2022).\n\nFAQS: VIIRS Modeled Fire Perimeter Product\n1. What is the VIIRS Modeled Fire Perimeter Product?\nA: The VIIRS Modeled Fire Perimeter Product uses the Fire Event Database Suite (FEDS) algorithm (Chen et al., 2022) to cluster VIIRS 375 m active fire detection data into individual fire events and track fire spread and behavior every 12 hours (Figure 1). The FEDS algorithm uses an alpha shape approach to model the fire perimeter, estimate the active portion of the fire perimeter, and quantify metrics of fire behavior for each 12-hour time step. The FEDS algorithm currently uses the 375 m active fire detections from the VIIRS sensors onboard the Suomi-NPP and NOAA-20 satellites to model the fire perimeter.\n\n\n\nFigure 1. A schematic diagram of the FEDS fire tracking algorithm (Chen et al., 2022). (a) Two idealized fire objects (Fire 1, Fire 2) at the time step t. Colored and grey dots represent newly detected and previously detected VIIRS 375 m active fire pixels, respectively. The brown segments along the fire perimeter are the modeled estimate of the active portions of the fire perimeter. (b) At the next time step t+1, new fire pixels (in red) are clustered (C1, C2, C3), and are used for forming a new fire object (C1 for Fire 3), or for the growth of an existing fire object (C2 for Fire 1). Expanded fire objects (Fire 2) are merged with an existing active fire object (Fire 1) if they grow close enough (due to the addition of C3). Vector shapes of the modeled fire perimeter and active fire line segments for each fire object are updated at each 12-hour time step.\n\n\nThe VIIRS Modeled Fire Perimeter data based on the FEDS algorithm includes three different types of information about each ongoing fire event in the Continental United States (CONUS) and Canada, updated approximately four hours after each VIIRS overpass: 1) the modeled perimeter of each large fire event, 2) the active portion of the modeled fire perimeter (i.e., the active fire line or “hot edge” of the modeled fire perimeter), and 3) the instantaneous fire radiative power (FRP) from all active fire detections associated with a given fire event during each overpass time, including fire detections at the hot edge and residual flaming or smoldering activity within the modeled fire perimeter (see Figure 1a). By clustering and tracking fire detections into individual fire events, the FEDS algorithm provides a consistent assessment of the change in fire behavior and intensity with each successive VIIRS overpass (nominally at 01:30 and 13:30 local time).\n2. What is the source of the VIIRS Modeled Fire Perimeter Data?\nA: The VIIRS Modeled Fire Perimeter data based on the FEDS algorithm are provided by the NASA Earth Information System (EIS) Project, which harnesses the full power of NASA’s satellite observations, models, and scientific expertise to deliver information for societal benefit that is open, accessible, and actionable. The FEDS algorithm was developed by Yang Chen and colleagues at the University of California-Irvine, NASA’s Goddard Space Flight Center (GSFC), Cardiff University (UK), and Universidad del Rosario (Colombia), as described in a scientific paper published in 2022 (Chen et al., 2022).\n3. What is the temporal frequency of the VIIRS Modeled FIre Perimeter Data?\nA: The VIIRS Modeled Fire Perimeter data based on the FEDS algorithm are updated twice per day using the VIIRS 375 m active fire information from the morning (01:30) and afternoon (13:30) Suomi-NPP and NOAA-20 satellite overpasses over each fire event. The vector output from the FEDS algorithm is produced within approximately 4 hours following the availability of VIIRS 375 m active fire data in FIRMS.\n4. What is the spatial resolution of the VIIRS Modeled Fire Perimeter data?\nA: Although the vector data in the VIIRS Modeled Fire Perimeter product has no defined spatial resolution, the product is derived from the VIIRS 375 m active fire detections, and therefore reflects the spatial resolution and geolocation accuracy of those products from the Suomi-NPP and NOAA-20 satellites. The FEDS algorithm uses an alpha hull approach to cluster new active fire detections and model the updated fire perimeter every 12 hours. The alpha hull approach groups individual fire detection points from the VIIRS sensors on two satellite platforms to model their combined spatial extent and distribution at each 12-hour time step. Compared to a convex hull, the alpha hull approach is better able to capture irregular shapes of active fire events and changes in shape over time. See Chen et al., (2022) for additional details.\n5. Why are the VIIRS Modeled Fire Perimeter Data based on the FEDS algorithm only available for the Continental United States (CONUS) and Canada?\nA: The FEDS algorithm was originally developed and tested for large fires in California (Chen et al., (2022)). The EIS Fire team has scaled the production of the fire event tracking approach to cover CONUS and Canada, with data available via OGC API. Data for additional regions, including Alaska and Hawaii, will be released through FIRMS when available.\n6. What is the temporal extent of VIIRS Modeled Fire Perimeter data in FIRMS?\nA: The VIIRS Modeled Fire Perimeter data based on the FEDS algorithm are currently available through FIRMS beginning January 1, 2024. Archive data for prior years is available via OGC API. This example notebook demonstrates how to subset, analyze, and visualize these data outside of FIRMS.\n7. What validation of the VIIRS Modeled Fire Perimeter data has been performed?\nA: Chen et al., (2022) provide a rigorous validation of the FEDS algorithm for large fire events in California between 2012-2020. The study compared the final FEDS perimeter data to official year-end fire perimeter data from the Fire and Resource Assessment Program (FRAP), established by the California Department of Forestry and Fire Protection. Table 6 in Chen et al. (2022) summarizes the comparison between FEDS and FRAP data for large fires in California in 2018 using standard comparison metrics (e.g., Accuracy, Precision, Recall, and Intersection Over Union). Overall, the FEDS data compare favorably to the FRAP year-end data. Validation for other regions and other fire types is an area of ongoing research by the EIS Fire Project team, and updates from these ongoing studies will be added, as available.\n8. How does the VIIRS Modeled Fire Perimeter Data product differ from the USA Fire Perimeter layers in FIRMS?\nA: The VIIRS Modeled Fire Perimeter Data product provides situational awareness for large fire events in CONUS and Canada every 12 hours, based on the FEDS algorithm and available 375 m VIIRS active fire detection data from Suomi-NPP and NOAA-20. The modeled perimeter is an estimate of the fire-affected area, active portion of the fire perimeter, and metrics of fire behavior. The VIIRS Modeled Fire Perimeter data also provides a history of modeled large fire growth every 12 hours for all fire events detected by VIIRS in CONUS and Canada. By contrast, the USA Fire Perimeter layer is the most recent official incident perimeter data. Official incident data provide a more precise estimate of the perimeter of large fire events in the US than the VIIRS Modeled Fire Perimeter data. The USA Fire Perimeter layer is updated periodically with new official incident perimeter data.\n9. What are the attributes of the VIIRS Modeled Fire Perimeter data based on the FEDS algorithm?\n\n\n\n\n\n\n\n\n\nColumn\nDescription\nType/Unit\nDisplayed on FIRMS\n\n\n\n\nfireid\nFire ID. Unique for each fire. Matches fireid.\nNumeric ID\n\n\n\npixden\nPixel density, the number of pixels divided by area of the estimated perimeter.\npixels/km2\n\n\n\nduration\nNumber of days since first observation of fire. Fires with a single observation have a duration of zero.\nDays\n\n\n\nflinelen\nLength of active fire line, based on new pixels. If no new pixels are detected, flinelen is set to zero.\nkm\n\n\n\nfperim\nLength of estimated fire perimeter.\nkm\n\n\n\nfarea\nArea within estimated fire perimeter.\nkm2\n\n\n\nn_newpixels\nNumber of pixels newly detected since last overpass.\npixels\n\n\n\nn_pixels\nNumber of pixel detections in history of fire.\npixels\n\n\n\nisactive\nHave new fire pixels been detected in the last 5 days?\nBoolean\n\n\n\nogc_fid\nThe ID used by the OGC API to sort perimeters.\nNumeric ID\n\n\n\ngeometry\nThe shape of the estimated perimeter.\nGeometry\n\n\n\n\n10. What is the rationale for the 5-day threshold on new fire pixels?\nA: The FEDS algorithm uses spatial and temporal search criteria to cluster VIIRS 375 m active fire detections into existing or new fire objects. The use of the 5-day threshold to associate new fire detections with an existing fire object reflects the nature of large fire behavior and periodic gaps in active fire detection data. Large fire events often exhibit episodic growth in response to changes in fire weather, suppression activity, and fuels. Periods with less intense fire activity may not generate active fire detections from VIIRS, leading to temporal gaps (12-hour, 24-hour, etc.) in fire detection information. The 5-day threshold allows the FEDS algorithm to connect future fire detections to the same fire object, or event, rather than splitting a large fire into multiple objects. In addition to periods of lower fire intensity, gaps in fire detection can occur due to satellite outages or observing conditions such as clouds that impact the VIIRS fire detection algorithm. The 5-day threshold reduces the influence of these data gaps on the fragmentation of large fire events in the FEDS algorithm output.\n11. How can I find more information about the FEDS algorithm, VIIRS Modeled Fire Perimeter data product versions, and related analyses?\nA: Additional information about the FEDS algorithm can be found in Chen et al., (2022). The source code used to generate the VIIRS Modeled Fire Perimeter data product is available on GitHub, and an example notebook that demonstrates how to subset, analyze, and visualize the FEDS output outside of FIRMS is available on NASA’s VEDA platform. Finally, the data products from the FEDS algorithm are also available via OGC API.",
    "crumbs": [
      "Data",
      "Data Products"
    ]
  },
  {
    "objectID": "docs/data_sources.html",
    "href": "docs/data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "TODO: explain data sources (e.g. VIIRS collections), latency, quality.",
    "crumbs": [
      "Data",
      "Data Sources"
    ]
  },
  {
    "objectID": "docs/contributing.html",
    "href": "docs/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Edit locally with VSCode Quarto extension:\n\nDownload Quarto\nInstall jupyter if needed (pip install jupyter)\nInstall Quarto extension in VSCode\nClone this repo and open it in VSCode.\nMake edits to .qmd, .md or .ipynb files.\n\n\n\n\n\n\n\nTip\n\n\n\nOpen the Command Palette (cmd-shift-p on Mac) and select Quarto: Edit in Visual Mode for What You See Is What You Get editing. Select Quarto: Edit in Source Mode to go back.\n\n\n\nPreview website. Select the Preview icon in upper right, use the keyboard shortcut cmd-shift-k, or use the Quarto: Preview command from the Command Palette.\nOnce everything looks good, commit your changes and push to GitHub. If you pushed directly to main, GitHub will automatically build and deploy the new website reflecting your changes. If you are working on a branch, GitHub will build and deploy your changes when you create a pull request and merge your branch into main.\n\nEdit in your browser\n\nFor simple edits, you can edit markdown (.qmd and .md) files directly on GitHub. Again, as soon as you push to main, the website will rebuild to reflect your changes.\n\nEdit in the MAAP ADE\n\nYou could also clone the repo in the ADE and use that interface to edit markdown and Jupyter Notebooks, then commit to GitHub the same way.\nPreview coming soon???\n\n\n\n\n$ git clone &lt;this-repo&gt; fireatlas\n$ cd fireatlas/\n$ pip install -e '.[test]'\n$ pytest -v tests/\n$ pytest -v --runslow tests/ # include long-running tests",
    "crumbs": [
      "Technical Documentation",
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#editing-docs",
    "href": "docs/contributing.html#editing-docs",
    "title": "Contributing",
    "section": "",
    "text": "Edit locally with VSCode Quarto extension:\n\nDownload Quarto\nInstall jupyter if needed (pip install jupyter)\nInstall Quarto extension in VSCode\nClone this repo and open it in VSCode.\nMake edits to .qmd, .md or .ipynb files.\n\n\n\n\n\n\n\nTip\n\n\n\nOpen the Command Palette (cmd-shift-p on Mac) and select Quarto: Edit in Visual Mode for What You See Is What You Get editing. Select Quarto: Edit in Source Mode to go back.\n\n\n\nPreview website. Select the Preview icon in upper right, use the keyboard shortcut cmd-shift-k, or use the Quarto: Preview command from the Command Palette.\nOnce everything looks good, commit your changes and push to GitHub. If you pushed directly to main, GitHub will automatically build and deploy the new website reflecting your changes. If you are working on a branch, GitHub will build and deploy your changes when you create a pull request and merge your branch into main.\n\nEdit in your browser\n\nFor simple edits, you can edit markdown (.qmd and .md) files directly on GitHub. Again, as soon as you push to main, the website will rebuild to reflect your changes.\n\nEdit in the MAAP ADE\n\nYou could also clone the repo in the ADE and use that interface to edit markdown and Jupyter Notebooks, then commit to GitHub the same way.\nPreview coming soon???",
    "crumbs": [
      "Technical Documentation",
      "Contributing"
    ]
  },
  {
    "objectID": "docs/contributing.html#running-tests",
    "href": "docs/contributing.html#running-tests",
    "title": "Contributing",
    "section": "",
    "text": "$ git clone &lt;this-repo&gt; fireatlas\n$ cd fireatlas/\n$ pip install -e '.[test]'\n$ pytest -v tests/\n$ pytest -v --runslow tests/ # include long-running tests",
    "crumbs": [
      "Technical Documentation",
      "Contributing"
    ]
  },
  {
    "objectID": "docs/regions.html",
    "href": "docs/regions.html",
    "title": "Regions",
    "section": "",
    "text": "CONUS, Boreal, Russia East, Hawaii? Bolivia?"
  },
  {
    "objectID": "docs/system_architecture.html",
    "href": "docs/system_architecture.html",
    "title": "NRT System Architecture",
    "section": "",
    "text": "In addition to the core FEDS algorithm, the team maintains a production system to track current fires in near-real time (NRT). This data is made available on our API. It ingests fire detection data from the VIIRS sensors on Suomi-NPP and NOAA20 satellites, runs the FEDS fire even tracking algorithm forward to account for these new observations, and outputs the updated data to the API.\nThis documentation describes the technical architecture of the production system, primarily for the benefit of our development team and partners.",
    "crumbs": [
      "Technical Documentation",
      "NRT System Architecture"
    ]
  },
  {
    "objectID": "docs/system_architecture.html#overview",
    "href": "docs/system_architecture.html#overview",
    "title": "NRT System Architecture",
    "section": "Overview",
    "text": "Overview\n\n\n\nProduction system architecture. Icons in black represent resources managed through this repo.\n\n\nData flows through the system from left to right. The LANCE NRT3 server provides low-latency access to incoming satellite data.\nTODO: details on what collections we take from.\nAll of the computational tasks are completed as jobs on the NASA-ESA Multi-Mission Algorithm and Analysis Platform’s Data Processing System: the DPS, for short. We use GitHub Actions to schedule and submit jobs to the DPS, as detailed in the next section.\n\nScheduled Jobs\nAt a high level, regularly scheduled jobs check for new NRT data from the LANCE server, then save it to the FEDSinput/ folder on our gsfc_landslides MAAP S3 shared bucket. This data is preprocessed, then stored in FEDSpreprocessed/NOAA20 or FEDSpreprocessed/SNPP, depending on its source.\nTwice a day, at 15:25 and 23:25 UTC, each scheduled region is run through the main FireForward algorithm to incorporate any new observations. The outputs from these runs are saved to FEDSoutputs-v3/ and copied to the VEDA S3 bucket (TODO- link to docs for feature API). This triggers a workflow that makes the outputs available on the VEDA API. (TODO link).\n\n\nManual Jobs\nSee How To Run A New Region for information about how you can manually create data for a new region or time period.",
    "crumbs": [
      "Technical Documentation",
      "NRT System Architecture"
    ]
  },
  {
    "objectID": "docs/system_architecture.html#actions",
    "href": "docs/system_architecture.html#actions",
    "title": "NRT System Architecture",
    "section": "Actions",
    "text": "Actions\nWe use GitHub Actions to drive the process of scheduling and submitting all computations in our production system, as well as for testing, releasing, and deploying our code. Below, we give explanations for each Action, which are in turn called by Workflows. Together, these Workflows coordinate the production system.\n\nrun-dps-job-v3\nIn a container based on the vanilla MAAP image (so that the maap Python package is available), run submit-dps-jobs.py. This calls the MAAP API and submits a job with the passed parameters.\n\n\n\n\n\n\nNote\n\n\n\nNote that when we submit a job to DPS, we include an algorithm version tag (github_ref) in addition to the algo_name parameter. This tells DPS which specific prebuilt image to use for that job. The version of the fireatlas code that is used in a particular image will always be the version it was originally built with, even if more commits have been made to the repo in the meantime.\nThis is important! See Releasing for more information.\n\n\nMost of our workflows use this action to submit different combinations of input parameters to the DPS.\nOnce DPS has created a container using the tagged image, it runs the run_dps_cli.sh script. Depending on the arguments passed, this will trigger one of:\n\nFireRunDaskCoordinator.py\nFireRunDataUpdateChecker.py\nFireRunPreprocessRegion.py\nFireRunByRegionAndT.py\nFireRunFireForward.py\n\nFireRunDaskCoordinator.py is the main coordinating program, and is where most users should start if attempting to run FEDS locally. In practice, it is also usually what runs on DPS.\n\n\nalert-on-failed-dps-jobs\nThis action spins up a container using the base MAAP image, uses the MAAP API to check if any jobs by certain users have failed in the past hour. If any have, the action lists them out and fails.\n\n\nrelease\nSee Releasing for more information.",
    "crumbs": [
      "Technical Documentation",
      "NRT System Architecture"
    ]
  },
  {
    "objectID": "docs/system_architecture.html#workflows",
    "href": "docs/system_architecture.html#workflows",
    "title": "NRT System Architecture",
    "section": "Workflows",
    "text": "Workflows\n\nschedule-alert-failed-dps-jobs\nRuns the alert-on-failed-dps-jobs action every hour. If a failure is detected, sends a notification to the #fireatlas-alerts channel in our Slack workspace. (This is neccessary because only the GitHub account that initiated the workflow will recieve an email notification if it fails.)\n\n\nmanual-v3\nUsed to kick off a one-time run on the production system. Useful for archival data generation and testing new features or datasets. See How To Run A New Region.\n\n\npytest\nUses pytest to automatically run test suite on any PR or pull to the main conus-dps branch. These include several integration tests that only run with the --runslow flag. These tests are slower and can take ~15 minutes to run.\n\n\n\n\n\n\nNote\n\n\n\nNote that one static landcover file for testing currently lives on the EIS-SMCE S3 public bucket (TODO address), as the GitHub Actions runner does not have permission to read landcover data from our private MAAP S3 bucket.\n\n\n\n\nschedule-{region}-nrt-v3\nOngoing jobs that update fire perimeters in the specified regions twice per day.\n\n\nrelease\nSee Releasing for more information.",
    "crumbs": [
      "Technical Documentation",
      "NRT System Architecture"
    ]
  }
]