[
  {
    "objectID": "docs/new_region.html",
    "href": "docs/new_region.html",
    "title": "New Regions",
    "section": "",
    "text": "Most custom regions will require their EPSG codes and other settings (stored in FireConsts.py:Settings) to change for runs. This doc explains how to change those settings and create a new scheduler v3 job to run on DPS.\n\n\nThere’s nothing special about changing these environment variables. All that is needed for this to work is to put a .env file in the Settings.PREPROCESSED_DIR region folder to be picked up. A couple points will describe how this all works:\n\nAll DPS jobs are kicked off via the /maap_runtime/run_dps_cli.sh script. In that file there’s a single function and line that attempts (and gracefully exists) to bring over the .env file for a DPS run:\n# TODO: this will have to be changed to be passed dynamically once we want to use other s3 buckets\ncopy_s3_object \"s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/${regnm}/.env\" ../fireatlas/.env\nWe use the package pydantic-settings in the fireatlas code to manage our settings. If you look at the class declaration in FireConts.py you’ll see this piece of config below. This tells us a few different things:\nclass Settings(BaseSettings):\n# read in all env vars prefixed with `FEDS_` they can be in a .env file\nmodel_config = {\n    \"env_file\": \".env\",\n    \"extra\": \"ignore\",\n    \"env_prefix\": \"FEDS_\",\n}\n\nif there is a .env file available read it and use the key/values there to override any defaults listed in this class\nif there are variables in the .env file that are not declared in this class do not use them\neverything in the .env that will act as an override is prefixed with FEDS_ (so to override LOCAL_PATH in the .env you would declare FEDS_LOCAL_PATH=&lt;value&gt;)\n\n\nThat’s it. Then any imports of the fireatlas package should now have these overrides including calls to python3 FireRunDaskCoordinator.py\n\n\n\nIn the future this should definitely be a more rigid (and only additive) workflow. But the fire time will need to define this more. For now there’s only a requirement to pick a decent sounding region name. The example below will use the newly created “RussiaEast” region to explain how this is done.\n\nPick a region name (e.g. “RussiaEast”)\nCreate a folder in Settings.PREPROCESSED_DIR with a .env file. You’ll probably be doing this in a MAAP or VEDA JupyterHub and depending on the bucket parameters things outside JH might be restricted. So from within JH you can use the pre-installed aws-cli tool to do this. The command below assumes you’ve already created a local .env file you’re going to copy:\n# just showing how to list bucket key contents\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/\n                       PRE BorealManualV2/\n                       PRE BorealNA/\n                       PRE CONUS/\n\n# what's the RussiaEast .env look like?\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# cat .env \nFEDS_FTYP_OPT=\"global\"\nFEDS_CONT_OPT=\"global\"\nFEDS_EPSG_CODE=6933\n\n\n# copy the .env to a new FEDSpreprocessed folder\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 cp /projects/.env s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\nupload: ./.env to s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\n\n# check that it exists \n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/\n2024-07-12 05:55:19         66 .env\nNow that the region is defined we can immediately test it out with the GH Action “manual-nrt-v3” workflow to make sure it works before we move on to scheduling it permanently. There will be more documentation about how to run this in the future. For now the only thing to mention is that the JSON encoded param input will look something like this. And remember to NOT pass it enclosed in single quotes or double quotes but rather naked:\n{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556,46.1226744036175,168.70469654881543,77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}\n\n\n\n\n\nFind an existing v3 scheduled worklow in the .github/workflows/*.yaml such as .github/workflows/schedule-conus-nrt-v3.yaml and copy it\ncp .github/workflows/schedule-conus-nrt-v3.yaml .github/workflows/schedule-russian-east-nrt-v3.yaml\nEdit the new workflow and make sure to change some of the following sections and values. Note that leaving tst or ted as [] means it will run from current year 01/01 to now:\nname: &lt;your region name&gt; nrt v3\n...\n    - name: kick off the DPS job\n   uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n   with:\n     algo_name: eis-feds-dask-coordinator-v3\n     github_ref: 1.0.0\n     username: gcorradini\n     queue: maap-dps-eis-worker-128gb\n     maap_image_env: ubuntu\n     maap_pgt_secret: ${{ secrets.MAAP_PGT }}\n     json_params: '{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556, 46.1226744036175, 168.70469654881543, 77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}'",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/new_region.html#how-dps-jobs-pick-up-custom-settings",
    "href": "docs/new_region.html#how-dps-jobs-pick-up-custom-settings",
    "title": "New Regions",
    "section": "",
    "text": "There’s nothing special about changing these environment variables. All that is needed for this to work is to put a .env file in the Settings.PREPROCESSED_DIR region folder to be picked up. A couple points will describe how this all works:\n\nAll DPS jobs are kicked off via the /maap_runtime/run_dps_cli.sh script. In that file there’s a single function and line that attempts (and gracefully exists) to bring over the .env file for a DPS run:\n# TODO: this will have to be changed to be passed dynamically once we want to use other s3 buckets\ncopy_s3_object \"s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/${regnm}/.env\" ../fireatlas/.env\nWe use the package pydantic-settings in the fireatlas code to manage our settings. If you look at the class declaration in FireConts.py you’ll see this piece of config below. This tells us a few different things:\nclass Settings(BaseSettings):\n# read in all env vars prefixed with `FEDS_` they can be in a .env file\nmodel_config = {\n    \"env_file\": \".env\",\n    \"extra\": \"ignore\",\n    \"env_prefix\": \"FEDS_\",\n}\n\nif there is a .env file available read it and use the key/values there to override any defaults listed in this class\nif there are variables in the .env file that are not declared in this class do not use them\neverything in the .env that will act as an override is prefixed with FEDS_ (so to override LOCAL_PATH in the .env you would declare FEDS_LOCAL_PATH=&lt;value&gt;)\n\n\nThat’s it. Then any imports of the fireatlas package should now have these overrides including calls to python3 FireRunDaskCoordinator.py",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/new_region.html#defining-a-new-region",
    "href": "docs/new_region.html#defining-a-new-region",
    "title": "New Regions",
    "section": "",
    "text": "In the future this should definitely be a more rigid (and only additive) workflow. But the fire time will need to define this more. For now there’s only a requirement to pick a decent sounding region name. The example below will use the newly created “RussiaEast” region to explain how this is done.\n\nPick a region name (e.g. “RussiaEast”)\nCreate a folder in Settings.PREPROCESSED_DIR with a .env file. You’ll probably be doing this in a MAAP or VEDA JupyterHub and depending on the bucket parameters things outside JH might be restricted. So from within JH you can use the pre-installed aws-cli tool to do this. The command below assumes you’ve already created a local .env file you’re going to copy:\n# just showing how to list bucket key contents\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/\n                       PRE BorealManualV2/\n                       PRE BorealNA/\n                       PRE CONUS/\n\n# what's the RussiaEast .env look like?\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# cat .env \nFEDS_FTYP_OPT=\"global\"\nFEDS_CONT_OPT=\"global\"\nFEDS_EPSG_CODE=6933\n\n\n# copy the .env to a new FEDSpreprocessed folder\n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 cp /projects/.env s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\nupload: ./.env to s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/.env\n\n# check that it exists \n(pangeo) root@workspaceqhqrmmz1pim87fsz:~# aws s3 ls s3://maap-ops-workspace/shared/gsfc_landslides/FEDSpreprocessed/RussiaEast/\n2024-07-12 05:55:19         66 .env\nNow that the region is defined we can immediately test it out with the GH Action “manual-nrt-v3” workflow to make sure it works before we move on to scheduling it permanently. There will be more documentation about how to run this in the future. For now the only thing to mention is that the JSON encoded param input will look something like this. And remember to NOT pass it enclosed in single quotes or double quotes but rather naked:\n{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556,46.1226744036175,168.70469654881543,77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/new_region.html#defining-a-new-region-schedule-run",
    "href": "docs/new_region.html#defining-a-new-region-schedule-run",
    "title": "New Regions",
    "section": "",
    "text": "Find an existing v3 scheduled worklow in the .github/workflows/*.yaml such as .github/workflows/schedule-conus-nrt-v3.yaml and copy it\ncp .github/workflows/schedule-conus-nrt-v3.yaml .github/workflows/schedule-russian-east-nrt-v3.yaml\nEdit the new workflow and make sure to change some of the following sections and values. Note that leaving tst or ted as [] means it will run from current year 01/01 to now:\nname: &lt;your region name&gt; nrt v3\n...\n    - name: kick off the DPS job\n   uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n   with:\n     algo_name: eis-feds-dask-coordinator-v3\n     github_ref: 1.0.0\n     username: gcorradini\n     queue: maap-dps-eis-worker-128gb\n     maap_image_env: ubuntu\n     maap_pgt_secret: ${{ secrets.MAAP_PGT }}\n     json_params: '{\"regnm\": \"RussiaEast\", \"bbox\": \"[97.16172779881556, 46.1226744036175, 168.70469654881543, 77.81982396998427]\", \"tst\": \"[2024,5,1,\\\"AM\\\"]\", \"ted\": \"[]\", \"operation\": \"--coordinate-all\"}'",
    "crumbs": [
      "Technical Documentation",
      "New Regions"
    ]
  },
  {
    "objectID": "docs/regions.html",
    "href": "docs/regions.html",
    "title": "Regions",
    "section": "",
    "text": "CONUS, Boreal, Russia East, Hawaii? Bolivia?"
  },
  {
    "objectID": "docs/api.html",
    "href": "docs/api.html",
    "title": "API",
    "section": "",
    "text": "Hello world!",
    "crumbs": [
      "Data",
      "API"
    ]
  },
  {
    "objectID": "docs/releasing.html",
    "href": "docs/releasing.html",
    "title": "Releasing",
    "section": "",
    "text": "In this context “releasing” means the folloing things:\n\ntagging the algorithm with a certain semantic version (semver for short)\nbuild and publishing fireatlas on PyPI\nbuilding an image off that tag that will be used in some async task runner (currently only DPS) to run the regional algorithm jobs asynchronously.\n\nMost of this can be automated but since semver is often about considering if the newest set of changes we are packaging up under a version is backward compatible it does require a human to choose the version.\nThat said, the fireatlas code isn’t a library that others will be using in their code and so that also relieves us of considering backward incompatible changes. Therefore, we will probably never increment the major release number and only minor or patch in &lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;. Another way of saying that is we fireatlas will always be releasing within the &gt;=1.0.0,&lt;2.0.0 range\n\n\n\nThe releaser should look at the current release tags and versions and decide if the minor or patch version should be incremented:\n\nare all the merged changes in this release just bug fixes? then bump the patch (1.&lt;minor&gt;.&lt;patch&gt;) version by one\ndid any of the merged changes going out include new features? then bump the minor (1.&lt;minor&gt;.&lt;patch&gt;) version by one\n\n\n\n\nOnce the releaser has a version number, then will need to create a PR that modifies version in a couple places:\n\nthe algorithm config algorithm_version in ./maap_runtime/coordinator/algorithm_config.yaml:\n\nalgorithm_description: \"coordinator for all regional jobs, preprocess and FireForward steps\"\nalgorithm_version: &lt;NEW VERSION NUMBER HERE&gt;\nenvironment: ubuntu\n\nunfortunately all the scheduled jobs also pass this version to kick off jobs and therefore also need to be updated in ./.github/workflows/schedule-*.yaml:\n\n- name: kick off the DPS job\n  uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n  with:\n    algo_name: eis-feds-dask-coordinator-v3\n    github_ref: &lt;NEW VERSION NUMBER HERE&gt;\n    username: gcorradini\n\n\n\nThe releaser can merge the above PR and then kick off a new release by doing the following:\n\nGo to https://github.com/Earth-Information-System/fireatlas/releases\nclick “Draft New Release”\ncreate a new tag for this release that matches the version chosen above\nclick the “Generate release notes”\nreview the release notes and clean up\nclick the “Publish release”\n\n\n\n\nThe manual step in the last section will kick off an async GH actions workflow that does the following\n\nuses our version information and builds a python package using twine\npublishes the package to PyPI with that version number\nkicks off a DPS job that builds a new image\n\n\n\n\nThe biggest thing that can wrong with this workflow is that the DPS image builder fails to build our image. Then the algorithm will not be running the newest code. In the GH release action job you should be able see in the logs where the DPS image job is building and check the status:\n# EXAMPLE LOG\n{\"code\": 200, \"message\": {\"id\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"short_id\": \"ec3202d4\", \"created_at\": \"2024-07-30T20:34:28.000+00:00\", \"parent_ids\": [\"91dfb3a4edff20c7049825101f015b67c8a05d3a\"], \"title\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"message\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"author_name\": \"root\", \"author_email\": \"root@845666954fdb\", \"authored_date\": \"2024-07-30T20:34:28.000+00:00\", \"committer_name\": \"root\", \"committer_email\": \"root@845666954fdb\", \"committed_date\": \"2024-07-30T20:34:28.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/commit/ec3202d4adeb02f7d887d88d2af9784184e60344\", \"stats\": {\"additions\": 7, \"deletions\": 7, \"total\": 14}, \"status\": \"created\", \"project_id\": 3, \"last_pipeline\": {\"id\": 14293, \"iid\": 1332, \"project_id\": 3, \"sha\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"ref\": \"main\", \"status\": \"created\", \"source\": \"push\", \"created_at\": \"2024-07-30T20:34:29.737Z\", \"updated_at\": \"2024-07-30T20:34:29.737Z\", \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/pipelines/14293\"}, \"job_web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578\", \"job_log_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578/raw\"}}\nPlease verify that the job succeeds or you’ll have to manually trigger an image build again via ./maap_runtime/register-all.ipynb. To verify that the job succeeded, you can view the DPS job’s progress in GitLab at the job_web_url provided in the response from DPS as shown above.",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#choose-a-version-number",
    "href": "docs/releasing.html#choose-a-version-number",
    "title": "Releasing",
    "section": "",
    "text": "The releaser should look at the current release tags and versions and decide if the minor or patch version should be incremented:\n\nare all the merged changes in this release just bug fixes? then bump the patch (1.&lt;minor&gt;.&lt;patch&gt;) version by one\ndid any of the merged changes going out include new features? then bump the minor (1.&lt;minor&gt;.&lt;patch&gt;) version by one",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#create-a-new-pr-for-dps-jobs",
    "href": "docs/releasing.html#create-a-new-pr-for-dps-jobs",
    "title": "Releasing",
    "section": "",
    "text": "Once the releaser has a version number, then will need to create a PR that modifies version in a couple places:\n\nthe algorithm config algorithm_version in ./maap_runtime/coordinator/algorithm_config.yaml:\n\nalgorithm_description: \"coordinator for all regional jobs, preprocess and FireForward steps\"\nalgorithm_version: &lt;NEW VERSION NUMBER HERE&gt;\nenvironment: ubuntu\n\nunfortunately all the scheduled jobs also pass this version to kick off jobs and therefore also need to be updated in ./.github/workflows/schedule-*.yaml:\n\n- name: kick off the DPS job\n  uses: Earth-Information-System/fireatlas/.github/actions/run-dps-job-v3@conus-dps\n  with:\n    algo_name: eis-feds-dask-coordinator-v3\n    github_ref: &lt;NEW VERSION NUMBER HERE&gt;\n    username: gcorradini",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#merge-pr-and-manually-release",
    "href": "docs/releasing.html#merge-pr-and-manually-release",
    "title": "Releasing",
    "section": "",
    "text": "The releaser can merge the above PR and then kick off a new release by doing the following:\n\nGo to https://github.com/Earth-Information-System/fireatlas/releases\nclick “Draft New Release”\ncreate a new tag for this release that matches the version chosen above\nclick the “Generate release notes”\nreview the release notes and clean up\nclick the “Publish release”",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#release-publish-workflow",
    "href": "docs/releasing.html#release-publish-workflow",
    "title": "Releasing",
    "section": "",
    "text": "The manual step in the last section will kick off an async GH actions workflow that does the following\n\nuses our version information and builds a python package using twine\npublishes the package to PyPI with that version number\nkicks off a DPS job that builds a new image",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/releasing.html#verify-dps-image-build",
    "href": "docs/releasing.html#verify-dps-image-build",
    "title": "Releasing",
    "section": "",
    "text": "The biggest thing that can wrong with this workflow is that the DPS image builder fails to build our image. Then the algorithm will not be running the newest code. In the GH release action job you should be able see in the logs where the DPS image job is building and check the status:\n# EXAMPLE LOG\n{\"code\": 200, \"message\": {\"id\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"short_id\": \"ec3202d4\", \"created_at\": \"2024-07-30T20:34:28.000+00:00\", \"parent_ids\": [\"91dfb3a4edff20c7049825101f015b67c8a05d3a\"], \"title\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"message\": \"Registering algorithm: eis-feds-dask-coordinator-v3\", \"author_name\": \"root\", \"author_email\": \"root@845666954fdb\", \"authored_date\": \"2024-07-30T20:34:28.000+00:00\", \"committer_name\": \"root\", \"committer_email\": \"root@845666954fdb\", \"committed_date\": \"2024-07-30T20:34:28.000+00:00\", \"trailers\": {}, \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/commit/ec3202d4adeb02f7d887d88d2af9784184e60344\", \"stats\": {\"additions\": 7, \"deletions\": 7, \"total\": 14}, \"status\": \"created\", \"project_id\": 3, \"last_pipeline\": {\"id\": 14293, \"iid\": 1332, \"project_id\": 3, \"sha\": \"ec3202d4adeb02f7d887d88d2af9784184e60344\", \"ref\": \"main\", \"status\": \"created\", \"source\": \"push\", \"created_at\": \"2024-07-30T20:34:29.737Z\", \"updated_at\": \"2024-07-30T20:34:29.737Z\", \"web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/pipelines/14293\"}, \"job_web_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578\", \"job_log_url\": \"https://repo.maap-project.org/root/register-job-hysds-v4/-/jobs/14578/raw\"}}\nPlease verify that the job succeeds or you’ll have to manually trigger an image build again via ./maap_runtime/register-all.ipynb. To verify that the job succeeded, you can view the DPS job’s progress in GitLab at the job_web_url provided in the response from DPS as shown above.",
    "crumbs": [
      "Technical Documentation",
      "Releasing"
    ]
  },
  {
    "objectID": "docs/playbook_dps_debug.html",
    "href": "docs/playbook_dps_debug.html",
    "title": "DPS Debugging Playbook",
    "section": "",
    "text": "DPS Debugging Playbook\nHere are a series of steps that can help find out where something is failing regardless of whether the job is \"completed\" or \"failed\" state from the DPS perspective.\n\nDPS Job UI/UX:\nAll resolution routes will eventually lead us to use the DPS Job UI/UX so let’s make sure we have this workspace set up in the ADE:\napiVersion: 1.0.0\nmetadata:\n  name: dps-job-management\nattributes:\n  editorFree: 'true'\ncomponents:\n  - endpoints:\n      - attributes:\n          type: ide\n          discoverable: 'false'\n          path: /\n          protocol: http\n          public: 'true'\n        name: jupyter\n        port: 3100\n    referenceContent: |\n      kind: List\n      items:\n        - apiVersion: v1\n          kind: Pod\n          metadata:\n            name: ws\n            labels:\n              ssh: enabled\n          spec:\n            containers:\n              - name: jupyter\n                image: 'mas.dit.maap-project.org/root/maap-workspaces/hysds-proxy:ops'\n                imagePullPolicy: Always\n                resources:\n                  limits:\n                    memory: 8096Mi\n    type: kubernetes\n\n\nI Think a Job is Failing:\nStep one, we need metadata about a job to find it in the DPS Job UI/UX above. Since GH Actions are the interface to kicking off all manual jobs or scheduled jobs then that’s where we’ll go to figure everything out.\nThere are two paths to resolution here. Both are expanded on in the sections below:\n\nfind the GH Action that kicked off the job\nfind any failing jobs that ended within the last hour using the GH Action \"schedule-alert-failed-dps-jobs\"\n\n\n\nGH Action That Kicked Off A Job\nIf used the manual GH Action to kick off a job or if the job was scheduled we can go find it in the GH Actions UI. Note that you can filter for all job runs of the job you want to find by clicking the left-handed nav highlighted in red below: \nFind the most recent scheduled job or the manual job you kicked off and then click through to see the job steps. The step that dumps out job metadata is called \"kick off DPS job\" as highlighted in red below. Also highlighted in red are the two most important pieces of metadata information to retrieve about the job: the algorithm name, algorithm version and the job id: \n\n\nGH Action Alert Failed DPS Jobs\nClick on the \"alert-failed-dps-jobs\" GH Action in the left-handed nav of actions to see most recent runs. This action runs every hour and sniffs for jobs that failed with an end time within the past hour. Actions that detect faild jobs should be red as opposed to green: \nClick on a red action and drill down to the step that dumps out job metadata is called \"filter DPS failed jobs and alert\" as highlighted in red below. Also highlighted in red are the two most important pieces of metadata information to retrieve about the job: the algorithm name, algorithm version and the job id: \n\n\nGo to DPS UI/UX And Filter For Job\nThe easiest way to find a job is using the algorithm name and version and then using the job id to verify its correct. Go to the DPS Job UI/UX and \nIf a DPS job is in the \"failed\" state it should have a clear traceback listed in the job card. If it is in the \"completed\" state then you should be able to click through in the job card under \"View Job\" and look at the _stderr.txt and running.log in outputs for more information",
    "crumbs": [
      "Technical Documentation",
      "DPS Debugging Playbook"
    ]
  },
  {
    "objectID": "docs/contributing.html",
    "href": "docs/contributing.html",
    "title": "Running Tests",
    "section": "",
    "text": "Running Tests\n$ git clone &lt;this-repo&gt; fireatlas\n$ cd fireatlas/\n$ pip install -e '.[test]'\n$ pytest -v tests/\n$ pytest -v --runslow tests/ # include long-running tests",
    "crumbs": [
      "Technical Documentation",
      "Running Tests"
    ]
  },
  {
    "objectID": "docs/about.html",
    "href": "docs/about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "Home/About"
    ]
  },
  {
    "objectID": "docs/system_architecture.html",
    "href": "docs/system_architecture.html",
    "title": "System Architecture",
    "section": "",
    "text": "In addition to the core FEDS algorithm, the team maintains a production system to track current fires in near-real time (NRT). This data is made available on our API. It ingests fire detection data from the VIIRS sensors on Suomi-NPP and NOAA20 satellites, runs the FEDS fire even tracking algorithm forward to account for these new observations, and outputs the updated data to the API.\nThis documentation describes the technical architecture of the production system, primarily for the benefit of our development team and partners.",
    "crumbs": [
      "Technical Documentation",
      "System Architecture"
    ]
  },
  {
    "objectID": "docs/system_architecture.html#overview",
    "href": "docs/system_architecture.html#overview",
    "title": "System Architecture",
    "section": "Overview",
    "text": "Overview\n\n\n\nProduction system architecture. Icons in black represent resources managed through this repo.\n\n\nData flows through the system from left to right. The LANCE NRT3 server provides low-latency access to incoming satellite data.\nTODO: details on what collections we take from.\nAll of the computational tasks are completed as jobs on the NASA-ESA Multi-Mission Algorithm and Analysis Platform’s Data Processing System: the DPS, for short. We use GitHub Actions to schedule and submit jobs to the DPS, as detailed in the next section.\n\nScheduled Jobs\nAt a high level, regularly scheduled jobs check for new NRT data from the LANCE server, then save it to the FEDSinput/ folder on our gsfc_landslides MAAP S3 shared bucket. This data is preprocessed, then stored in FEDSpreprocessed/NOAA20 or FEDSpreprocessed/SNPP, depending on its source.\nTwice a day, at 15:25 and 23:25 UTC, each scheduled region is run through the main FireForward algorithm to incorporate any new observations. The outputs from these runs are saved to FEDSoutputs-v3/ and copied to the VEDA S3 bucket (TODO- link to docs for feature API). This triggers a workflow that makes the outputs available on the VEDA API. (TODO link).\n\n\nManual Jobs\nSee How To Run A New Region for information about how you can manually create data for a new region or time period.",
    "crumbs": [
      "Technical Documentation",
      "System Architecture"
    ]
  },
  {
    "objectID": "docs/system_architecture.html#actions",
    "href": "docs/system_architecture.html#actions",
    "title": "System Architecture",
    "section": "Actions",
    "text": "Actions\nWe use GitHub Actions to drive the process of scheduling and submitting all computations in our production system, as well as for testing, releasing, and deploying our code. Below, we give explanations for each Action, which are in turn called by Workflows. Together, these Workflows coordinate the production system.\n\nrun-dps-job-v3\nIn a container based on the vanilla MAAP image (so that the maap Python package is available), run submit-dps-jobs.py. This calls the MAAP API and submits a job with the passed parameters.\n\n\n\n\n\n\nNote\n\n\n\nNote that when we submit a job to DPS, we include an algorithm version tag (github_ref) in addition to the algo_name parameter. This tells DPS which specific prebuilt image to use for that job. The version of the fireatlas code that is used in a particular image will always be the version it was originally built with, even if more commits have been made to the repo in the meantime.\nThis is important! See Releasing for more information.\n\n\nMost of our workflows use this action to submit different combinations of input parameters to the DPS.\nOnce DPS has created a container using the tagged image, it runs the run_dps_cli.sh script. Depending on the arguments passed, this will trigger one of:\n\nFireRunDaskCoordinator.py\nFireRunDataUpdateChecker.py\nFireRunPreprocessRegion.py\nFireRunByRegionAndT.py\nFireRunFireForward.py\n\nFireRunDaskCoordinator.py is the main coordinating program, and is where most users should start if attempting to run FEDS locally. In practice, it is also usually what runs on DPS.\n\n\nalert-on-failed-dps-jobs\nThis action spins up a container using the base MAAP image, uses the MAAP API to check if any jobs by certain users have failed in the past hour. If any have, the action lists them out and fails.\n\n\nrelease\nSee Releasing for more information.",
    "crumbs": [
      "Technical Documentation",
      "System Architecture"
    ]
  },
  {
    "objectID": "docs/system_architecture.html#workflows",
    "href": "docs/system_architecture.html#workflows",
    "title": "System Architecture",
    "section": "Workflows",
    "text": "Workflows\n\nschedule-alert-failed-dps-jobs\nRuns the alert-on-failed-dps-jobs action every hour. If a failure is detected, sends a notification to the #fireatlas-alerts channel in our Slack workspace. (This is neccessary because only the GitHub account that initiated the workflow will recieve an email notification if it fails.)\n\n\nmanual-v3\nUsed to kick off a one-time run on the production system. Useful for archival data generation and testing new features or datasets. See How To Run A New Region.\n\n\npytest\nUses pytest to automatically run test suite on any PR or pull to the main conus-dps branch. These include several integration tests that only run with the --runslow flag. These tests are slower and can take ~15 minutes to run.\n\n\n\n\n\n\nNote\n\n\n\nNote that one static landcover file for testing currently lives on the EIS-SMCE S3 public bucket (TODO address), as the GitHub Actions runner does not have permission to read landcover data from our private MAAP S3 bucket.\n\n\n\n\nschedule-{region}-nrt-v3\nOngoing jobs that update fire perimeters in the specified regions twice per day.\n\n\nrelease\nSee Releasing for more information.",
    "crumbs": [
      "Technical Documentation",
      "System Architecture"
    ]
  }
]